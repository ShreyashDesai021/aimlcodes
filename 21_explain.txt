Block-level overview (what each section does & why)

Block 1 ‚Äî Imports & config: load libraries (NumPy/Pandas/Matplotlib + scikit-learn metrics/preprocessing/split), set a fixed random seed so results are reproducible, and peek at the raw CSV. 

21 - cancer

Block 2 ‚Äî Load & preprocess (no leakage): load the Breast Cancer Wisconsin dataset, drop non-features (IDs), detect the target column automatically, map labels to numbers (Malignant=1, Benign=0), split into features/labels, do a stratified train/test split, then standardize features using train-only statistics (prevents data leakage). 

21 - cancer

Block 3 ‚Äî Polynomial kernel: define 
ùêæ
(
ùë•
,
ùëß
)
=
(
ùõæ
‚Äâ
ùë•
‚ãÖ
ùëß
+
ùëê
0
)
ùëë
K(x,z)=(Œ≥x‚ãÖz+c
0
	‚Äã

)
d
, the building block that lets a linear model behave non-linearly via the ‚Äúkernel trick.‚Äù 

21 - cancer

Block 4 ‚Äî SVM with polynomial kernel (minimal SMO): implement a small, educational SVM trainer (dual form) that optimizes the Lagrange multipliers 
ùõº
Œ± with a simplified SMO procedure, tracks support vectors, and exposes decision_function and predict. 

21 - cancer

Block 5 ‚Äî Train SVM: choose hyperparameters (C, degree, 
ùõæ
Œ≥, 
ùëê
0
c
0
	‚Äã

), fit on training data, and produce predictions and decision scores on the test set. 

21 - cancer

Block 6 ‚Äî Evaluation & visualization: compute confusion matrix, Accuracy/Precision/Recall/F1, plot a confusion-matrix heatmap, compute the ROC curve/AUC from scores (not hard labels), and draw the ROC curve. 

21 - cancer

Block 7 ‚Äî Quick tweaks: try a few polynomial degrees to see how accuracy changes (very useful for a fast sensitivity check). 

21 - cancer

Line-by-line explanation (simple terms + key concepts)
Block 1: Imports & global config
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


Load NumPy (fast arrays & linear algebra), Pandas (tables, CSV I/O), and Matplotlib (plots). 

21 - cancer

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    confusion_matrix, roc_curve, auc,
    classification_report, accuracy_score,
    precision_score, recall_score, f1_score
)


Tools for splitting data, scaling features, and computing classification metrics (confusion matrix + Accuracy/Precision/Recall/F1; ROC curve + AUC). ROC needs a continuous score, not hard labels. 

21 - cancer

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)


Fix the random seed so shuffles/splits are reproducible (same results every run). 

21 - cancer

data = pd.read_csv("C:/Users/dshre/Downloads/Breast Cancer Wisconsin (Diagnostic)_21.csv")
print(data.columns)
data.head()


Read the CSV once to quickly inspect column names and first rows (sanity check). (Later, the file is read again in Block 2 for the actual pipeline.) 

21 - cancer

Block 2: Load & preprocess dataset (NO leakage)
data = pd.read_csv("C:/Users/dshre/Downloads/Breast Cancer Wisconsin (Diagnostic)_21.csv")


Load the dataset ‚Äúfor real‚Äù to start the clean pipeline. 

21 - cancer

data = data.drop(columns=['id', 'Unnamed: 0'], errors='ignore')


Drop identifier columns that aren‚Äôt predictive. errors='ignore' avoids crashing if a column is missing. 

21 - cancer

target_col_candidates = ['y', 'diagnosis', 'target', 'label', 'class']
target_col = next((c for c in target_col_candidates if c in data.columns), None)
if target_col is None:
    raise ValueError(f"Could not find a target column among {target_col_candidates}")


Auto-detect the target among common names. If none is found, fail early with a clear error. (Defensive coding.) 

21 - cancer

data[target_col] = data[target_col].map({'M': 1, 'B': 0}).fillna(data[target_col]).astype(int)
data = data.rename(columns={target_col: 'y'})


Map string labels to numbers: Malignant = 1, Benign = 0. If the column was already numeric, fillna(...original...) preserves it. Then rename to a standard y. 

21 - cancer

X = data.drop(columns=['y']).values.astype(float)
y = data['y'].values.astype(int)


Split into features X (all other columns) and labels y (0/1). Convert to numeric arrays for fast math. 

21 - cancer

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
)


Stratified split: preserves class ratios (benign/malignant) in train and test; test size is 20%. Reproducible with the same seed. 

21 - cancer

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)


Standardize: fit mean/std on train only (avoids data leakage), apply to both sets. Scaling stabilizes optimization and keeps features comparable. 

21 - cancer

print("‚úÖ Data ready")
print("Train shape:", X_train.shape, "| Test shape:", X_test.shape)
print("Class balance (train):", np.bincount(y_train), " (test):", np.bincount(y_test))


Quick confirmation: shapes and class counts per split (sanity check for stratification). 

21 - cancer

Block 3: Polynomial kernel
def polynomial_kernel(X1, X2, degree=3, gamma=1.0, coef0=1.0):
    """
    K(x, z) = (gamma * x¬∑z + coef0) ** degree
    Shapes:
      X1: (n1, d), X2: (n2, d) -> returns (n1, n2)
    """
    return (gamma * np.dot(X1, X2.T) + coef0) ** degree


Defines the kernel function. It computes all pairwise dot-products between rows of X1 and X2, then applies 
(
ùõæ
‚ãÖ
+
ùëê
0
)
ùëë
(Œ≥‚ãÖ+c
0
	‚Äã

)
d
. This implicitly maps inputs to a higher-dimensional space without creating those features explicitly. 

21 - cancer

Block 4: SVM with Polynomial Kernel (minimal SMO)
class SVMPolynomial:
    def __init__(self, C=1.0, degree=3, gamma=1.0, coef0=1.0, tol=1e-3, max_passes=10, seed=42):
        self.C = float(C)
        self.degree = int(degree)
        self.gamma = float(gamma)
        self.coef0 = float(coef0)
        self.tol = float(tol)
        self.max_passes = int(max_passes)
        self.seed = int(seed)
        self.b = 0.0


Store hyperparameters: C (regularization), degree/Œ≥/coef0 (kernel), tol/max_passes (stopping), RNG seed, and bias b. (Higher C penalizes margin violations more ‚Üí potentially less margin, lower training error; too high risks overfitting.) 

21 - cancer

    def _kernel(self, X1, X2):
        return polynomial_kernel(X1, X2, self.degree, self.gamma, self.coef0)


Internal helper to call the kernel with the model‚Äôs hyperparameters. 

21 - cancer

    def fit(self, X, y):
        y = np.where(y <= 0, -1.0, 1.0).astype(float)
        m = X.shape[0]
        alphas = np.zeros(m, dtype=float)
        b = 0.0


Convert labels to {-1,+1} for SVM math, set number of samples m, initialize Lagrange multipliers 
ùõº
ùëñ
Œ±
i
	‚Äã

 and bias 
ùëè
b to zero. Concept: SVM‚Äôs dual form optimizes 
ùõº
Œ±, not w directly. 

21 - cancer

        K = self._kernel(X, X)


Precompute the kernel matrix 
ùêæ
ùëñ
ùëó
=
ùêæ
(
ùë•
ùëñ
,
ùë•
ùëó
)
K
ij
	‚Äã

=K(x
i
	‚Äã

,x
j
	‚Äã

) (speeds up many repeated kernel calls). 

21 - cancer

        def E(i):
            return (np.sum(alphas * y * K[:, i]) + b) - y[i]


Error for sample i: model score 
ùëì
(
ùë•
ùëñ
)
=
‚àë
ùëó
ùõº
ùëó
ùë¶
ùëó
ùêæ
ùëó
ùëñ
+
ùëè
f(x
i
	‚Äã

)=‚àë
j
	‚Äã

Œ±
j
	‚Äã

y
j
	‚Äã

K
ji
	‚Äã

+b minus true label 
ùë¶
ùëñ
y
i
	‚Äã

. Used to check KKT conditions and guide updates. 

21 - cancer

        passes = 0
        rng = np.random.default_rng(self.seed)


passes counts how many whole passes had no updates (stopping criterion), and rng is our random generator. 

21 - cancer

        while passes < self.max_passes:
            num_changed = 0
            for i in range(m):
                Ei = E(i)
                if ((y[i]*Ei < -self.tol and alphas[i] < self.C) or
                    (y[i]*Ei >  self.tol and alphas[i] > 0.0)):


Loop over all samples; for each i, compute error Ei and check KKT violation (conditions that tell us if 
ùõº
ùëñ
Œ±
i
	‚Äã

 is optimal). If violated, try to update 
ùõº
ùëñ
,
ùõº
ùëó
Œ±
i
	‚Äã

,Œ±
j
	‚Äã

. 

21 - cancer

                    j = i
                    while j == i:
                        j = rng.integers(0, m)
                    Ej = E(j)


Pick a different sample j at random (common simple SMO heuristic), compute its error. 

21 - cancer

                    ai_old, aj_old = alphas[i], alphas[j]


Save old values for later delta computations. 

21 - cancer

                    if y[i] != y[j]:
                        L = max(0.0, aj_old - ai_old)
                        H = min(self.C, self.C + aj_old - ai_old)
                    else:
                        L = max(0.0, ai_old + aj_old - self.C)
                        H = min(self.C, ai_old + aj_old)
                    if L == H:
                        continue


Compute box constraints 
[
ùêø
,
ùêª
]
[L,H] for 
ùõº
ùëó
Œ±
j
	‚Äã

 given 
ùõº
ùëñ
Œ±
i
	‚Äã

 (from the equality constraint 
‚àë
ùõº
ùëñ
ùë¶
ùëñ
=
0
‚àëŒ±
i
	‚Äã

y
i
	‚Äã

=0). If the range collapses, skip. 

21 - cancer

                    eta = 2.0*K[i, j] - K[i, i] - K[j, j]
                    if eta >= 0:
                        continue


Œ∑ relates to the second derivative (curvature). In standard SMO, if Œ∑ ‚â• 0, the objective won‚Äôt improve by changing 
ùõº
ùëó
Œ±
j
	‚Äã

, so skip. 

21 - cancer

                    alphas[j] -= y[j] * (Ei - Ej) / eta
                    alphas[j] = np.clip(alphas[j], L, H)
                    if abs(alphas[j] - aj_old) < 1e-5:
                        continue


Proposed unconstrained update for 
ùõº
ùëó
Œ±
j
	‚Äã

, then clip to 
[
ùêø
,
ùêª
]
[L,H]. If the change is tiny, skip (no useful progress). 

21 - cancer

                    alphas[i] += y[i]*y[j]*(aj_old - alphas[j])


Update the paired 
ùõº
ùëñ
Œ±
i
	‚Äã

 to maintain the equality constraint. 

21 - cancer

                    b1 = (b - Ei
                          - y[i]*(alphas[i]-ai_old)*K[i, i]
                          - y[j]*(alphas[j]-aj_old)*K[i, j])
                    b2 = (b - Ej
                          - y[i]*(alphas[i]-ai_old)*K[i, j]
                          - y[j]*(alphas[j]-aj_old)*K[j, j])


Two candidate bias updates (derived from KKT optimality when 
ùõº
ùëñ
Œ±
i
	‚Äã

 or 
ùõº
ùëó
Œ±
j
	‚Äã

 is at the margin). 

21 - cancer

                    if 0.0 < alphas[i] < self.C:
                        b = b1
                    elif 0.0 < alphas[j] < self.C:
                        b = b2
                    else:
                        b = 0.5*(b1 + b2)


Standard SMO rule: if either 
ùõº
Œ± is a free support vector (between 0 and C), use its corresponding bias; else average them. 

21 - cancer

                    num_changed += 1


Count updates this pass. 

21 - cancer

            passes = passes + 1 if num_changed == 0 else 0


If no alphas changed in this sweep, increment passes; otherwise reset. Stop when passes reaches max_passes. 

21 - cancer

        self.alphas = alphas
        self.b = b
        self.X_train = X
        self.y_train = y
        self.support_ = np.where(alphas > 1e-8)[0]


Save learned parameters and cache support vectors (where 
ùõº
>
0
Œ±>0). Concept: Only support vectors matter in the decision function. 

21 - cancer

    def decision_function(self, X):
        K = self._kernel(X, self.X_train)
        return K @ (self.alphas * self.y_train) + self.b


Compute scores 
ùëì
(
ùë•
)
=
‚àë
ùëñ
ùõº
ùëñ
ùë¶
ùëñ
ùêæ
(
ùë•
,
ùë•
ùëñ
)
+
ùëè
f(x)=‚àë
i
	‚Äã

Œ±
i
	‚Äã

y
i
	‚Äã

K(x,x
i
	‚Äã

)+b for new inputs. These continuous scores are essential for ROC curves. 

21 - cancer

    def predict(self, X):
        scores = self.decision_function(X)
        return (scores >= 0).astype(int), scores


Turn scores into hard labels (‚â•0 ‚Üí class 1; else 0) and also return the scores. 

21 - cancer

Block 5: Train SVM (tuneable hyperparams)
n_features = X_train.shape[1]
svm_poly = SVMPolynomial(
    C=1.0,
    degree=3,
    gamma=1.0 / n_features,
    coef0=1.0,
    tol=1e-3,
    max_passes=10,
    seed=RANDOM_STATE
)


Heuristic: set \gamma=1/\text{#features} as a sensible starting point; degree=3 is a common polynomial choice; C=1 is defaultish. 

21 - cancer

svm_poly.fit(X_train, y_train)
y_pred, y_scores = svm_poly.predict(X_test)


Train on standardized train data, then get labels and scores on the test set. 

21 - cancer

print("‚úÖ Training complete")
print(f"Support vectors: {len(svm_poly.support_)}")


Report completion and the count of support vectors (a feel for model sparsity/complexity). 

21 - cancer

Block 6: Evaluation (CM, metrics, ROC + viz)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()


Build confusion matrix and unpack True/False Positives/Negatives. Layout: [[TN, FP],[FN, TP]]. 

21 - cancer

print("Confusion Matrix:\n", cm)
print(f"TN={tn}, FP={fp}, FN={fn}, TP={tp}")
print("Accuracy (%):", accuracy_score(y_test, y_pred) * 100.0)
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1:", f1_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, digits=4))


Print core metrics:

Accuracy = (TP+TN)/All

Precision = TP/(TP+FP) (how pure positives are)

Recall = TP/(TP+FN) (how many positives we caught)

F1 = harmonic mean of precision & recall.

Classification report gives per-class metrics and macro/weighted averages. 

21 - cancer

fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)
print(f"AUC: {roc_auc:.4f}")


ROC needs scores; compute False-Positive/True-Positive rates across thresholds and the Area Under Curve (AUC) ‚Äî higher is better (1.0 perfect, 0.5 random). 

21 - cancer

plt.figure(figsize=(5, 4))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
...
plt.show()


Draw a Confusion-Matrix heatmap, annotate counts, set axis labels ‚ÄúBenign/ Malignant.‚Äù Visual inspection helps spot error types (e.g., many FNs would be concerning for cancer). 

21 - cancer

plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, lw=2, label=f'Polynomial SVM (AUC = {roc_auc:.3f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
...
plt.show()


ROC curve: model curve vs the diagonal chance line; the farther above the diagonal, the better. 

21 - cancer

Block 7 (Optional): Quick tweaks
for deg in [2, 3, 4]:
    svm_poly = SVMPolynomial(C=1.0, degree=deg, gamma=1.0/X_train.shape[1], coef0=1.0, tol=1e-3, max_passes=12, seed=RANDOM_STATE)
    svm_poly.fit(X_train, y_train)
    y_pred, y_scores = svm_poly.predict(X_test)
    print(f"Degree={deg} | Accuracy={accuracy_score(y_test, y_pred):.4f}")


Quick hyperparameter sweep on the kernel degree to see how Accuracy reacts. In a real project, you‚Äôd grid-search C/degree/Œ≥ with cross-validation, but this gives fast intuition. 

21 - cancer